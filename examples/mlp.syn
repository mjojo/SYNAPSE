// mlp.syn - Multi-Layer Perceptron Forward Pass
// Architecture: [3 Inputs] -> [2 Hidden] -> [1 Output]

// --- MATH KERNEL ---

fn dot(a, b, len) {
    let sum = 0
    let i = 0
    while (i < len) {
        let p = a[i] * b[i]
        let sum = sum + p
        let i = i + 1
    }
    return sum
}

fn relu(x) {
    if (x < 0) {
        return 0
    }
    return x
}

// --- LAYER ENGINE ---

fn layer_forward(inputs, W, B, out, n_in, n_out) {
    let j = 0
    while (j < n_out) {
        let w_row = W[j]
        let z = dot(inputs, w_row, n_in)
        let b_val = B[j]
        let z = z + b_val
        let activated = relu(z)
        out[j] = activated
        let j = j + 1
    }
    return 0
}

fn main() {
    print(1111)
    
    // ==========================================
    // LAYER 1 CONFIG (3 Inputs -> 2 Neurons)
    // ==========================================
    let n_in_1 = 3
    let n_out_1 = 2
    
    // Inputs: [1, 2, 3]
    let X = alloc(n_in_1)
    X[0] = 1
    X[1] = 2
    X[2] = 3
    
    // Hidden Output Buffer
    let H = alloc(n_out_1)
    
    // Weights 1 (matrix of pointers)
    let W1 = alloc(n_out_1)
    
    // Neuron 1.0: [1, 1, 1], Bias 0 -> dot = 6, relu(6) = 6
    let w1_0 = alloc(n_in_1)
    w1_0[0] = 1
    w1_0[1] = 1
    w1_0[2] = 1
    W1[0] = w1_0
    
    // Neuron 1.1: [10, -10, 0], Bias -10 -> dot = -10, z = -20, relu = 0
    let w1_1 = alloc(n_in_1)
    w1_1[0] = 10
    w1_1[1] = 0 - 10
    w1_1[2] = 0
    W1[1] = w1_1
    
    // Biases 1
    let B1 = alloc(n_out_1)
    B1[0] = 0
    B1[1] = 0 - 10
    
    // ==========================================
    // LAYER 2 CONFIG (2 Inputs -> 1 Neuron)
    // ==========================================
    let n_in_2 = 2
    let n_out_2 = 1
    
    // Final Output Buffer
    let Y = alloc(n_out_2)
    
    // Weights 2 (matrix of pointers)
    let W2 = alloc(n_out_2)
    
    // Neuron 2.0: receives H=[6,0], weights=[2,3], bias=1
    // Calc: 6*2 + 0*3 + 1 = 13
    let w2_0 = alloc(n_in_2)
    w2_0[0] = 2
    w2_0[1] = 3
    W2[0] = w2_0
    
    // Biases 2
    let B2 = alloc(n_out_2)
    B2[0] = 1
    
    // ==========================================
    // EXECUTION GRAPH
    // ==========================================
    
    print(2001)
    // X -> [Layer 1] -> H
    layer_forward(X, W1, B1, H, n_in_1, n_out_1)
    
    // Debug: check hidden layer outputs
    let h0 = H[0]
    print(h0)
    let h1 = H[1]
    print(h1)
    
    print(2002)
    // H -> [Layer 2] -> Y
    layer_forward(H, W2, B2, Y, n_in_2, n_out_2)
    
    print(3333)
    let result = Y[0]
    print(result)
    
    return 0
}
